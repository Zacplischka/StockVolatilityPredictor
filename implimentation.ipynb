{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f5e9; border-left: 6px solid #4caf50; padding: 15px; border-radius: 5px;\">\n",
    "    <h1 style=\"color:#2e7d32;\">FIT5149 2024 S2 Assignment 1</h1>\n",
    "\n",
    "    Python Environment: 3.10.0\n",
    "\n",
    "    Libraries required can be found in the requirements.txt file.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e3f2fd; border-left: 6px solid #2196f3; padding: 15px; border-radius: 5px; width: fit-content;\">\n",
    "    <h2 style=\"color:#1976d2; margin-bottom: 5px;\">Student Information</h2>\n",
    "    <p style=\"color:#333; font-size: 16px; margin: 0;\"><strong>Student ID:</strong> 26921677</p>\n",
    "    <p style=\"color:#333; font-size: 16px; margin: 0;\"><strong>Name:</strong> Zachary Plischka</p>\n",
    "    <p style=\"color:#333; font-size: 16px; margin: 0;\"><strong>Group:</strong> 70</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "# turn warnings off\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital Data Exploration and Analyisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "kag = pd.read_csv('data/A1_stock_volatility_kaggle.csv')\n",
    "df = pd.read_csv('data/A1_stock_volatility_labeled.csv')\n",
    "sub = pd.read_csv('data/A1_stock_volatility_submission.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df['target'] = df.groupby('stock')['volatility'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pandas profiling to get an overview of the data and the distribution of the features\n",
    "# profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the columns that are snapshot and cumulative, also monthly features and columns that are right skewed\n",
    "snapshot_quarterly = ['revenue', 'net income', 'gross profit', 'eps', 'total assets', 'total liabilities', 'total equity', 'cash and cash equivalents']\n",
    "cumulative_quarterly = ['operating cash flow', 'investing cash flow', 'financing cash flow']\n",
    "monthly_features = ['volatility', 'open', 'close', 'high', 'low', 'volume', 'amount', 'avg_price', 'return']\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "right_skewed = ['open', 'close', 'high', 'gross profit', 'revenue', 'low', 'volume', 'amount', 'avg_price', 'net income', \"total assets\", \"total liabilities\", \"total equity\", \"cash and cash equivalents\", 'operating cash flow']\n",
    "negative_cols = df[num_cols].loc[:, (df[num_cols] < 0).any()].columns.tolist()\n",
    "ltc = [x for x in right_skewed if x not in negative_cols] # log transform columns\n",
    "original_cols = df.columns\n",
    "\n",
    "def preprocessing(data, remove_outliers=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the given data by performing the following steps:\n",
    "    1. Creates a copy of the data.\n",
    "    2. If remove_outliers is True, deals with outliers by performing log transformation on specified columns, replacing extreme values with medians, and dropping rows with extreme values.\n",
    "    3. Creates lagged features for volatility and return.\n",
    "    4. Fills missing values in lagged features with the mean volatility and return for each stock.\n",
    "    5. Creates high_low_ratio feature.\n",
    "    6. Calculates rolling mean for volatility.\n",
    "    7. Fills missing values in volume_pct_change with the mean value for each stock.\n",
    "    8. Calculates P/E ratio.\n",
    "    9. Calculates log of volume.\n",
    "    10. Calculates volatility change.\n",
    "    11. Replaces infinite values with NaN.\n",
    "    12. Creates test_df by selecting the last row for each stock.\n",
    "    13. Drops rows with missing values from df.\n",
    "    14. Returns the preprocessed df and test_df.\n",
    "    Parameters:\n",
    "    - data: DataFrame, the input data to be preprocessed.\n",
    "    - remove_outliers: bool, indicates whether to remove outliers or not. Default is False.\n",
    "    Returns:\n",
    "    - df: DataFrame, the preprocessed data.\n",
    "    - test_df: DataFrame, the test data consisting of the last row for each stock.\n",
    "    \"\"\"\n",
    "\n",
    "    df = data.copy()\n",
    "    \n",
    "    # if remove_outliers is True, this will deal with outliers\n",
    "    if remove_outliers:\n",
    "        def log_transform_cols(data, cols):\n",
    "            for col in cols:\n",
    "                data[col] = np.log(data[col])\n",
    "            return data\n",
    "        \n",
    "        df = log_transform_cols(df, ltc)\n",
    "        df['volatility'] = np.where(df['volatility'] > 100, df['volatility'].median(), df['volatility'])\n",
    "        df['return'] = np.where(df['return'] > 150, df['return'].median(), df['return'])\n",
    "        df = df.drop(df[(df['volatility'] > 100) | (df['target'] > 100)].index)\n",
    "    \n",
    "    \n",
    "    # Create lagged features\n",
    "    df['vola_lag2'] = df.groupby('stock')['volatility'].shift(1)\n",
    "    df['vola_lag3'] = df.groupby('stock')['volatility'].shift(2)\n",
    "\n",
    "    # Fill missing values with the mean volatility for each stock\n",
    "    df['vola_lag2'] = df.groupby('stock')['vola_lag2'].transform(lambda x: x.fillna(x.mean()))\n",
    "    df['vola_lag3'] = df.groupby('stock')['vola_lag3'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    # Create lagged return features\n",
    "    df['return_lag2'] = df.groupby('stock')['return'].shift(1)\n",
    "    df['return_lag3'] = df.groupby('stock')['return'].shift(2)\n",
    "\n",
    "    # fill na with mean\n",
    "    df['return_lag2'] = df.groupby('stock')['return_lag2'].transform(lambda x: x.fillna(x.mean()))\n",
    "    df['return_lag3'] = df.groupby('stock')['return_lag3'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    # Create high_low_ratio feature\n",
    "    df['high_low_ratio'] = df['high'] / df['low']\n",
    "\n",
    "    # ROLLING FEATURES\n",
    "    df['vola_rolling_3'] = df.groupby('stock')['volatility'].rolling(3).mean().reset_index(0, drop=True)\n",
    "    # fill na with mean\n",
    "    df['vola_rolling_3'] = df.groupby('stock')['vola_rolling_3'].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    df['volume_pct_change'] = df.groupby('stock')['volume'].pct_change()\n",
    "    df['volume_pct_change'] = df.groupby('stock')['volume_pct_change'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    # P/E RATIO\n",
    "    df['pe_ratio'] = df['avg_price'] / df['eps']\n",
    "\n",
    "    df['log_volume'] = np.log(df['volume'])\n",
    "\n",
    "    df['volatility_change'] = (df['volatility'] - df['vola_lag2']).abs() + (df['vola_lag2'] - df['vola_lag3']).abs()\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    \n",
    "    #------------------------------------------------ Final Drop and creation of Test_df\n",
    "\n",
    "    \n",
    "    test_df = df.groupby('stock').tail(1)\n",
    "    df = df.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f7ff; color:#000000; padding:15px; border-radius:5px; border: 1px solid #cce7ff;\">\n",
    "  \n",
    "## Initial EDA and Beginning of Analysis Task\n",
    "\n",
    "Initial analysis of the dataset shows that there are some significant outliers in the dataset that need to be dealt with accordingly. Additionally, the dataset contains variables that are highly correlated with each other. This can be seen in both the correlation tab and the 'alerts' tab in the HTML output above.\n",
    "\n",
    "Columns that are highly skewed include:\n",
    "- open\n",
    "- close\n",
    "- high\n",
    "- low\n",
    "- avg_price\n",
    "- volatility\n",
    "- target\n",
    "\n",
    "The missing values detected in the dataset are in the 'target' column. This is because the target column is created by using next month's volatility. Since the last month of the dataset does not have a next month, the target column is missing a value for the last month. Therefore, this is the value that we ultimately want to predict. Since the data in the same row is not missing and will ideally contain historical information, we can use this data to predict the target value.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Comparing Outliers dealt with vs NOT dealt with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealt_with, _ = preprocessing(df, remove_outliers=True)\n",
    "not_dealt_with, _ = preprocessing(df, remove_outliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "def truncate_label(label, max_length=40):\n",
    "    return label[:max_length] + '...' if len(label) > max_length else label\n",
    "\n",
    "def plot_features_grid(df, target, cols_per_row=4):\n",
    "    \"\"\"Plot a grid of scatter plots showing the relationship between features and a target variable.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing the data.\n",
    "    - target (str): The name of the target variable.\n",
    "    - cols_per_row (int, optional): The number of columns per row in the grid. Default is 4.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\" \n",
    "    features = [col for col in df.columns if col != target]\n",
    "    n_features = len(features)\n",
    "    n_rows = math.ceil(n_features / cols_per_row)\n",
    "    \n",
    "    # Increase figure size and adjust subplot parameters\n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(5*cols_per_row, 4*n_rows))\n",
    "    fig.suptitle(f'Features vs {target}', fontsize=16)\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    \n",
    "    axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        sns.scatterplot(x=feature, y=target, data=df, ax=ax)\n",
    "        ax.set_title(truncate_label(feature), fontsize=10)\n",
    "        ax.set_xlabel(truncate_label(feature), fontsize=8)\n",
    "        ax.set_ylabel(target, fontsize=8)\n",
    "        \n",
    "        # Limit the number of x-ticks and truncate their labels\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax.set_xticklabels([truncate_label(label.get_text(), 10) for label in ax.get_xticklabels()])\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use the function\n",
    "plot_features_grid(not_dealt_with, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d3e0ea;\">\n",
    "  \n",
    "## Analysis of Outliers\n",
    "\n",
    "As we can see in the plots above, there is a clear outlier that has a target value of ~140, which is noticeable in every plot. Since the target variable was derived by shifting the volatility variable, we can also observe a few outliers in the volatility-related plots. Additionally, there are two potential outliers in the 'return' derived plots such as 'return_lag2', 'return_lag3', and 'pe_ratio'.\n",
    "\n",
    "Regarding the price-related variables such as 'open', 'close', 'high', 'low', and 'avg_price', there is a distinct cluster of potential outliers with prices significantly higher than the rest of the dataset. Upon further exploration, it appears that these data points are related to Walmart (\"WMT\") stock. These outliers can be addressed by applying transformations to the features. A potential transformation is the log transformation, which could compress larger values and reduce the right skew of the data.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dealt_with[not_dealt_with['open']> 300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dealt_with[not_dealt_with['volatility_change']> 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of dataset with outliers dealt with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_grid(dealt_with, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f1f8e9; color:#000000; padding:15px; border-radius:5px; border-left: 6px solid #81c784; padding: 15px;\">\n",
    "\n",
    "## Dealing with Outliers vs Not Dealing with Outliers:\n",
    "As we can see from the example below, the baseline model appears to perform better on the dataset where the outliers are dealt with, as opposed to leaving them in the dataset.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model and compare the performance with and without removing outliers\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "not_removed, not_removed_test = preprocessing(df, remove_outliers=False)\n",
    "removed, removed_test = preprocessing(df, remove_outliers=True)\n",
    "\n",
    "X_not_removed = not_removed.drop(columns=['target', 'stock', 'date'])\n",
    "y_not_removed = not_removed['target']\n",
    "\n",
    "X_removed = removed.drop(columns=['target', 'stock', 'date'])\n",
    "y_removed = removed['target']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_not_removed, X_val_not_removed, y_train_not_removed, y_val_not_removed = train_test_split(X_not_removed, y_not_removed, test_size=0.2, random_state=42)\n",
    "X_train_removed, X_val_removed, y_train_removed, y_val_removed = train_test_split(X_removed, y_removed, test_size=0.2, random_state=42)\n",
    "\n",
    "# train linear regression model\n",
    "lr_not_removed = LinearRegression()\n",
    "lr_removed = LinearRegression()\n",
    "\n",
    "lr_not_removed.fit(X_train_not_removed, y_train_not_removed)\n",
    "lr_removed.fit(X_train_removed, y_train_removed)\n",
    "\n",
    "# make predictions\n",
    "y_pred_not_removed = lr_not_removed.predict(X_val_not_removed)\n",
    "y_pred_removed = lr_removed.predict(X_val_removed)\n",
    "\n",
    "# evaluate model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_not_removed = mean_squared_error(y_val_not_removed, y_pred_not_removed)\n",
    "mse_removed = mean_squared_error(y_val_removed, y_pred_removed)\n",
    "\n",
    "print(f'MSE not removed: {mse_not_removed}')\n",
    "print(f'MSE removed: {mse_removed}')\n",
    "\n",
    "# r2 score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_not_removed = r2_score(y_val_not_removed, y_pred_not_removed)\n",
    "r2_removed = r2_score(y_val_removed, y_pred_removed)\n",
    "\n",
    "print(f'R2 not removed: {r2_not_removed}')\n",
    "print(f'R2 removed: {r2_removed}')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot predictions vs actual\n",
    "def plot_predictions_vs_actual(y_true, y_pred, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_predictions_vs_actual(y_val_not_removed, y_pred_not_removed, 'Predictions vs Actual (Not Removed)')\n",
    "plot_predictions_vs_actual(y_val_removed, y_pred_removed, 'Predictions vs Actual (Removed)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e8f4f8; color:#000000; padding:15px; border-radius:5px; border: 1px solid #b0d4de;\">\n",
    "  \n",
    "Dealing with the outliers in an appropriate manner has reduced the variance of our predictions, as shown by the decrease in the $R^2$ score and subsequently the decrease in the MSE.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f9f9f9; color:#000000; padding:15px; border-radius:5px; border: 1px solid #dddddd;\">\n",
    "\n",
    "## Begin Polynomial and Interaction Feature Engineering:\n",
    "\n",
    "The formation of polynomial features allows us to capture non-linear relationships between the input variables and the target variable. By creating new features here, the model can learn more complex patterns in the data.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# show max columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "kag = pd.read_csv('data/A1_stock_volatility_kaggle.csv')\n",
    "df = pd.read_csv('data/A1_stock_volatility_labeled.csv')\n",
    "sub = pd.read_csv('data/A1_stock_volatility_submission.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "df\n",
    "\n",
    "df['target'] = df.groupby('stock')['volatility'].shift(-1)\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing + Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "\n",
    "def create_polynomial_features(data, degree=2, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Generate polynomial features for the given data.\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame): The input data.\n",
    "    - degree (int): The degree of the polynomial features. Default is 2.\n",
    "    - exclude_columns (list): List of column names to exclude from polynomial feature generation. Default is None.\n",
    "    Returns:\n",
    "    - result (pandas.DataFrame): The data with polynomial features generated.\n",
    "    \"\"\"\n",
    "    \n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "    \n",
    "    # Separate excluded columns\n",
    "    excluded_data = data[exclude_columns]\n",
    "    features = data.drop(columns=exclude_columns)\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    poly_features = poly.fit_transform(features)\n",
    "    \n",
    "    # Check if get_feature_names_out method exists (newer scikit-learn versions)\n",
    "    if hasattr(poly, 'get_feature_names_out'):\n",
    "        poly_feature_names = poly.get_feature_names_out(features.columns)\n",
    "    # Fall back to get_feature_names for older versions\n",
    "    elif hasattr(poly, 'get_feature_names'):\n",
    "        poly_feature_names = poly.get_feature_names(features.columns)\n",
    "    else:\n",
    "        # If neither method is available, create generic feature names\n",
    "        n_features = poly_features.shape[1]\n",
    "        poly_feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "    \n",
    "    df_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=features.index)\n",
    "    \n",
    "    # Combine polynomial features with excluded columns\n",
    "    result = pd.concat([df_poly, excluded_data], axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the transformations to the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and create polynomial features\n",
    "processed_df, test_df = preprocessing(df, remove_outliers=True)\n",
    "\n",
    "# For df: exclude 'stock', 'date', and 'target', then add back 'stock' and 'target'\n",
    "df_poly = create_polynomial_features(processed_df, degree=2, exclude_columns=['stock', 'date', 'target'])\n",
    "df_poly = df_poly.drop(columns='date')  # Remove 'date' if it's not needed\n",
    "\n",
    "# For test_df: exclude 'stock' and 'date', then add back 'stock'\n",
    "test_df = create_polynomial_features(test_df, degree=2, exclude_columns=['stock', 'date', 'target'])\n",
    "test_df = test_df.drop(columns='date')  # Remove 'date' if it's not needed\n",
    "\n",
    "print(df_poly.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and analysing the new features that have been created using scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def plot_features(df, target, cols_per_row=4):\n",
    "    \"\"\"\n",
    "    Plot the feature analysis against the target variable.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing the features and target variable.\n",
    "    - target (str): The name of the target variable.\n",
    "    - cols_per_row (int, optional): The number of columns per row in the subplots. Default is 4.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    features = [col for col in df.columns if col != target]\n",
    "    n_features = len(features)\n",
    "    n_rows = math.ceil(n_features / cols_per_row)\n",
    "    \n",
    "    # Reduce the figure size\n",
    "    fig, axes = plt.subplots(n_rows, cols_per_row, figsize=(16, 4 * n_rows))\n",
    "    fig.suptitle(f'Feature Analysis vs {target}', fontsize=16)\n",
    "    \n",
    "    for i, col in enumerate(features):\n",
    "        row = i // cols_per_row\n",
    "        col_pos = i % cols_per_row\n",
    "        ax = axes[row, col_pos] if n_rows > 1 else axes[col_pos]\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # For numerical columns\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            \n",
    "            sns.scatterplot(data=df[~outliers], x=col, y=target, color='blue', alpha=0.6, ax=ax, s=10)\n",
    "            sns.scatterplot(data=df[outliers], x=col, y=target, color='red', alpha=0.6, ax=ax, s=10)\n",
    "        else:\n",
    "            # For categorical columns\n",
    "            sns.boxplot(data=df, x=col, y=target, ax=ax)\n",
    "        \n",
    "        ax.set_title(f'{col} vs {target}', fontsize=10)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(i+1, n_rows * cols_per_row):\n",
    "        row = j // cols_per_row\n",
    "        col_pos = j % cols_per_row\n",
    "        fig.delaxes(axes[row, col_pos] if n_rows > 1 else axes[col_pos])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_features(df_poly, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly analysing the visuals above, we can see that some new features have a strong linear correlation with the target, this can potentially improve the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f4f6fa; color:#000000; padding:15px; border-radius:5px; border: 1px solid #c9d3e0;\">\n",
    "\n",
    "## Feature Selection:\n",
    "\n",
    "Instead of using MSE or $R^2$ to evaluate feature selection, we use BIC. This is because MSE decreases monotonically as feature size increases, and $R^2$ increases monotonically as feature size increases.\n",
    "\n",
    "Below, we conduct forward feature selection on the transformed dataset using a simple linear regression model.\n",
    "\n",
    "**NOTE**: This cell can take upwards of 20 minutes to run. You can skip this cell and use the best features that were extracted below.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_bic(y_true, y_pred, n_features):\n",
    "    \"\"\"\n",
    "    Calculate the Bayesian Information Criterion (BIC) for a regression model.\n",
    "    Parameters:\n",
    "    - y_true (array-like): The true values of the target variable.\n",
    "    - y_pred (array-like): The predicted values of the target variable.\n",
    "    - n_features (int): The number of features used in the regression model.\n",
    "    Returns:\n",
    "    - bic (float): The calculated BIC value.\n",
    "    Notes:\n",
    "    - The BIC is a criterion for model selection among a finite set of models.\n",
    "    - It penalizes models with more features to prevent overfitting.\n",
    "    - The BIC is calculated as n_samples * log(mse) + n_features * log(n_samples),\n",
    "      where mse is the mean squared error between y_true and y_pred.\n",
    "    - If the mean squared error is less than or equal to 0, the BIC is set to infinity.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(y_true)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    if mse <= 0:\n",
    "        return np.inf\n",
    "    bic = n_samples * np.log(mse) + n_features * np.log(n_samples)\n",
    "    return bic\n",
    "\n",
    "# Split the data\n",
    "X = df_poly.drop(columns=['target', 'stock']).reset_index(drop=True)\n",
    "y = df_poly['target']\n",
    "\n",
    "# Create a pipeline with a standard scaler and a linear regression model\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# Initialize lists to store results\n",
    "n_features_list = []\n",
    "bic_scores = []\n",
    "selected_features_list = []\n",
    "\n",
    "# Initialize set of selected features\n",
    "selected_features = set()\n",
    "remaining_features = set(X.columns)\n",
    "\n",
    "# Perform forward feature selection\n",
    "for n_features in range(1, 101):\n",
    "    best_bic = np.inf\n",
    "    best_feature = None\n",
    "    \n",
    "    # Try adding each remaining feature\n",
    "    for feature in remaining_features:\n",
    "        current_features = list(selected_features) + [feature]\n",
    "        X_selected = X[current_features]\n",
    "        \n",
    "        # Evaluate the model with the current set of features\n",
    "        pipeline.fit(X_selected, y)\n",
    "        y_pred = pipeline.predict(X_selected)\n",
    "        bic = calculate_bic(y, y_pred, n_features)\n",
    "        \n",
    "        # Update best feature if this one gives a lower BIC\n",
    "        if bic < best_bic:\n",
    "            best_bic = bic\n",
    "            best_feature = feature\n",
    "    \n",
    "    # Add the best feature to the selected set\n",
    "    selected_features.add(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    \n",
    "    # Store the results\n",
    "    n_features_list.append(n_features)\n",
    "    bic_scores.append(best_bic)\n",
    "    selected_features_list.append(list(selected_features))\n",
    "    \n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Selected features: {list(selected_features)}\")\n",
    "    print(f\"BIC Score: {best_bic:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results = pd.DataFrame({\n",
    "    'n_features': n_features_list,\n",
    "    'bic': bic_scores,\n",
    "    'selected_features': selected_features_list\n",
    "})\n",
    "\n",
    "# Find the best number of features\n",
    "best_n_features = results.loc[results['bic'].idxmin(), 'n_features']\n",
    "best_features = results.loc[results['bic'].idxmin(), 'selected_features']\n",
    "\n",
    "print(f\"\\nBest number of features: {best_n_features}\")\n",
    "print(f\"Best features: {best_features}\")\n",
    "print(f\"Best BIC: {results['bic'].min():.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['n_features'], results['bic'], marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.title('Forward Feature Selection: BIC vs Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f5f7fb; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d0d7e5;\">\n",
    "\n",
    "## Analysis of Feature Selection:\n",
    "\n",
    "As shown in the above plot, there is a dramatic decrease in the BIC score from 0 to ~40 features, where the scoring begins to plateau. The best number of features that resulted from the forward feature selection was 54, with a BIC of 9.91.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['return_lag2 high_low_ratio',\n",
    " 'revenue vola_lag3',\n",
    " 'return_lag3 vola_rolling_3',\n",
    " 'volatility volume_pct_change',\n",
    " 'return volatility',\n",
    " 'volume vola_lag2',\n",
    " 'cash and cash equivalents vola_lag2',\n",
    " 'eps return_lag3',\n",
    " 'return_lag2 volatility_change',\n",
    " 'log_volume volatility_change',\n",
    " 'return_lag3 high_low_ratio',\n",
    " 'return vola_lag3',\n",
    " 'return cash and cash equivalents',\n",
    " 'low return',\n",
    " 'vola_rolling_3',\n",
    " 'vola_lag3 log_volume',\n",
    " 'avg_price volatility_change',\n",
    " 'amount revenue',\n",
    " 'vola_lag2 vola_lag3',\n",
    " 'total liabilities vola_rolling_3',\n",
    " 'close volatility_change',\n",
    " 'return investing cash flow',\n",
    " 'vola_lag2 return_lag3',\n",
    " 'vola_rolling_3 volatility_change',\n",
    " 'return return_lag2',\n",
    " 'return gross profit',\n",
    " 'volatility vola_lag2',\n",
    " 'volatility volatility_change',\n",
    " 'volume vola_lag3',\n",
    " 'volatility investing cash flow',\n",
    " 'return return_lag3',\n",
    " 'low volatility_change',\n",
    " 'cash and cash equivalents vola_rolling_3',\n",
    " 'return_lag2^2',\n",
    " 'return_lag3^2',\n",
    " 'return total liabilities',\n",
    " 'volatility vola_rolling_3',\n",
    " 'high volatility_change',\n",
    " 'high_low_ratio vola_rolling_3',\n",
    " 'volatility return_lag2',\n",
    " 'investing cash flow^2',\n",
    " 'avg_price return',\n",
    " 'close return',\n",
    " 'revenue vola_lag2',\n",
    " 'total assets volatility_change',\n",
    " 'vola_rolling_3^2',\n",
    " 'total assets vola_rolling_3',\n",
    " 'return^2',\n",
    " 'open volatility_change']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fafc; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d6dee7;\">\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Using the best features obtained from the feature selection, we set up a grid search, random search, and Bayesian search to tune the hyperparameters of the different models we wish to train.\n",
    "\n",
    "**Note**: Ideally, we would conduct feature selection on each model after training them. However, due to limited computational resources, this approach seemed optimal.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import uniform, randint\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Split data\n",
    "X = df_poly[best_features]\n",
    "y = df_poly['target']\n",
    "\n",
    "pipelines = {\n",
    "    'ridge': make_pipeline(StandardScaler(), Ridge(random_state=42)),\n",
    "    'xgb': XGBRegressor(random_state=42),  # Remove StandardScaler for XGBoost\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestRegressor(random_state=42)),\n",
    "    'svr': make_pipeline(StandardScaler(), SVR()),\n",
    "    'knn': make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "}\n",
    "\n",
    "# Create alphas\n",
    "alphas = np.logspace(-4, 4, 9)\n",
    "\n",
    "grid = {\n",
    "    'ridge': {\n",
    "        'ridge__alpha': alphas,\n",
    "    },\n",
    "    'rf': {\n",
    "        'randomforestregressor__n_estimators': [100, 200],\n",
    "        'randomforestregressor__max_depth': [3, 5, 7],\n",
    "    },\n",
    "    'svr': {\n",
    "        'svr__C': [1, 10],\n",
    "        'svr__kernel': ['rbf']\n",
    "    },\n",
    "    'knn': {\n",
    "        'kneighborsregressor__n_neighbors': [3, 5, 7],\n",
    "    }\n",
    "}\n",
    "\n",
    "# XGBoost optimized search parameters\n",
    "xgb_param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "}\n",
    "\n",
    "xgb_search_spaces = {\n",
    "    'n_estimators': Integer(100, 1000),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'subsample': Real(0.6, 1.0),\n",
    "    'colsample_bytree': Real(0.6, 1.0),\n",
    "    'min_child_weight': Integer(1, 10),\n",
    "    'gamma': Real(0, 0.5),\n",
    "    'reg_alpha': Real(0, 1),\n",
    "    'reg_lambda': Real(0, 1),\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Starting model fitting and evaluation...\")\n",
    "fit_models = {}\n",
    "\n",
    "for algo in pipelines.keys():\n",
    "    print(f\"Fitting {algo}...\")\n",
    "    \n",
    "    if algo == 'xgb':\n",
    "        # Step 1: Initial exploration with RandomizedSearchCV\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=pipelines[algo],\n",
    "            param_distributions=xgb_param_distributions,\n",
    "            n_iter=50,  \n",
    "            cv=3,\n",
    "            verbose=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        random_search.fit(X_train, y_train)\n",
    "        print(\"Best parameters found by RandomizedSearchCV:\")\n",
    "        print(random_search.best_params_)\n",
    "        \n",
    "        # Step 2: Fine-tuning with BayesianOptimization\n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=XGBRegressor(random_state=42),\n",
    "            search_spaces=xgb_search_spaces,\n",
    "            n_iter=25, \n",
    "            cv=3,\n",
    "            verbose=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        model = bayes_search\n",
    "    else:\n",
    "        model = GridSearchCV(pipelines[algo], grid[algo], cv=3, n_jobs=-1, verbose=1)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    fit_models[algo] = model\n",
    "    print(f'{algo} RMSE: {mse}')\n",
    "    print(f'Best parameters for {algo}: {model.best_params_}')\n",
    "    print(\"--------------------\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rmse for each model\n",
    "for algo, model in fit_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'{algo} RMSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f9fbfd; color:#000000; padding:15px; border-radius:5px; border: 1px solid #cfd7e2;\">\n",
    "\n",
    "## Selecting the Best Model:\n",
    "\n",
    "After tuning the hyperparameters on the models and employing 3-fold cross-validation throughout the process, the best performing model was the Ridge Regressor with an alpha value of 10.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f4f8fc; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d0d8e5;\">\n",
    "\n",
    "## Feature Importance:\n",
    "\n",
    "In this section, I will perform feature importance analysis to identify the key features that the models are utilizing.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# plot feature importances\n",
    "for algo, model in fit_models.items():\n",
    "    if algo not in ['xgb', 'rf', 'ridge']:\n",
    "        continue  \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Feature Importances ({algo.upper()})\")\n",
    "    \n",
    "    if algo in ['xgb', 'rf']:\n",
    "        if algo == 'xgb':\n",
    "            importances = model.best_estimator_.feature_importances_\n",
    "        elif algo == 'rf':\n",
    "            importances = model.best_estimator_.named_steps['randomforestregressor'].feature_importances_\n",
    "    elif algo == 'ridge':\n",
    "        importances = np.abs(model.best_estimator_.named_steps['ridge'].coef_)\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [best_features[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot r2 score for each model\n",
    "from sklearn.metrics import r2_score\n",
    "r2_scores = {}\n",
    "for algo, model in fit_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2_scores[algo] = r2_score(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(r2_scores.keys(), r2_scores.values())\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('R2 Score for Each Model')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f8fbff; color:#000000; padding:15px; border-radius:5px; border: 1px solid #cfd9e6;\">\n",
    "\n",
    "### Feature Importance Analysis:\n",
    "\n",
    "Based on the analysis above, volatility-related features are observed to be the most important features used by the models. Specifically, the rolling averages of the past 3 months stood out, as well as its squared counterpart. This can be demonstrated by the larger coefficients observed in the Ridge Regression model and the feature importance scores extracted from the XGB and RandomForest models.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7faff; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d3dcea;\">\n",
    "\n",
    "## Residual Analysis\n",
    "\n",
    "**THINGS TO LOOK FOR**\n",
    "\n",
    "- **Patterns**: Look for any distinct patterns in the residuals.\n",
    "  \n",
    "- **Spread**: The spread of residuals should be fairly consistent across the range of each feature. Inconsistencies might indicate heteroscedasticity.\n",
    "\n",
    "- **Outliers**: Identify any points far from the main cluster, especially if they appear in multiple plots.\n",
    "\n",
    "- **Trend Lines**: Ideally, each trend line should be close to horizontal.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Get the Ridge model and make predictions\n",
    "ridge_model = fit_models['ridge']\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Function to create a single residual plot\n",
    "def plot_residuals(ax, x, residuals, title):\n",
    "    \"\"\"\n",
    "    Plot the residuals against a given variable.\n",
    "    Parameters:\n",
    "    - ax (matplotlib.axes.Axes): The axes to plot on.\n",
    "    - x (array-like): The variable to plot against the residuals.\n",
    "    - residuals (array-like): The residuals to be plotted.\n",
    "    - title (str): The title of the plot.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.scatter(x, residuals, alpha=0.5, s=10)\n",
    "    ax.axhline(y=0, color='r', linestyle='-', linewidth=1)\n",
    "    ax.set_xlabel(title, fontsize=8)\n",
    "    ax.set_ylabel('Residuals', fontsize=8)\n",
    "    ax.set_title(f'Residuals vs {title}', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    \n",
    "    # Add a trend line\n",
    "    z = np.polyfit(x, residuals, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(x, p(x), \"r--\", alpha=0.8, linewidth=1)\n",
    "\n",
    "# Calculate number of rows needed (4 plots per row)\n",
    "n_features = len(X_test.columns) + 2  # +2 for predicted and actual values\n",
    "n_rows = math.ceil(n_features / 4)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(n_rows, 4, figsize=(20, 5*n_rows))\n",
    "fig.suptitle('Residual Plots for Ridge Model', fontsize=16)\n",
    "\n",
    "# Flatten axs if it's a 2D array\n",
    "axs = axs.flatten() if n_rows > 1 else axs\n",
    "\n",
    "# Plot residuals for each feature\n",
    "for i, feature in enumerate(X_test.columns):\n",
    "    plot_residuals(axs[i], X_test[feature], residuals, feature)\n",
    "\n",
    "# Plot residuals vs predicted values\n",
    "plot_residuals(axs[-2], y_pred, residuals, 'Predicted Target')\n",
    "\n",
    "# Plot residuals vs actual values\n",
    "plot_residuals(axs[-1], y_test, residuals, 'Actual Target')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for i in range(n_features, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)  # Adjust to prevent title overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f5f9ff; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d0d8e5;\">\n",
    "\n",
    "## Residual Analysis:\n",
    "\n",
    "Overall, the visualizations of the residuals are relatively good as most of the trend lines are very close to horizontal, and not too many outliers are present. This indicates a favorable environment for a linear regression model.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f5f9ff; color:#000000; padding:15px; border-radius:5px; border: 1px solid #d0d8e5;\">\n",
    "\n",
    "## Make predictions and save submission file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fit_models['ridge']\n",
    "pred_df = test_df[best_features]\n",
    "sub['Volatility'] = best_model.predict(pred_df)\n",
    "if 'Date' in sub.columns:\n",
    "    sub.drop(columns=['Date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('pred_values.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
